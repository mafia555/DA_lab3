# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #3 выполнил:
- Пикулин Марк Андреевич
- РИ-210933
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | # | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
### В данной лабораторной работе мы создадим ML-агент и будем тренировать нейросеть, задача которой будет заключаться в управлении шаром. Задача шара заключается в том, чтобы оставаясь на плоскости находить кубик, смещающийся в заданном случайном диапазоне координат.

-	Создим новый пустой 3D проект на Unity.
![10](https://user-images.githubusercontent.com/104256775/209389542-32a5bfea-3bfa-45f0-9ac8-a3cc21b0b014.png)

-	Скачаем папку с ML агентом. 
![20](https://user-images.githubusercontent.com/104256775/209389576-d821bd54-e78a-41bf-afb1-4251ef012b4f.png)


-	В созданный проект добавляем ML Agent, выбрав Window - Package Manager - Add Package from disk. Последовательно добавляем .json – файлы:
	ml-agents-release_19 / com,unity.ml-agents / package.json
	ml-agents-release_19 / com,unity.ml-agents.extensions / package.json
- Основные действия в Anaconda Prompt
![30](https://user-images.githubusercontent.com/104256775/209389604-fbebb358-b60c-4306-8643-dbeff11332d0.png)

-	Далее пишем серию команд для создания и активации нового ML-агента, а также для скачивания необходимых библиотек:

![40](https://user-images.githubusercontent.com/104256775/209389629-1e122cb1-b6cc-4b0b-8f20-2bc1b6391d92.png)

![41](https://user-images.githubusercontent.com/104256775/209389649-183dcc45-aaa1-4c49-9a8d-3dc30a2f1559.png)

-	Создадим на сцене плоскость, куб и сферу, а затем простой C# скрипт и подключим его к сфере:

![50](https://user-images.githubusercontent.com/104256775/209389696-af0f095c-f12f-452e-922c-ce68f0c9b2a2.png)

-	В скрипт-файл RollerAgent.cs добавляем код:


```C#

using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using Unity.MLAgents;
using Unity.MLAgents.Sensors;
using Unity.MLAgents.Actuators;

public class RollerAgent : Agent
{
    Rigidbody rBody;
    // Start is called before the first frame update
    void Start()
    {
        rBody = GetComponent<Rigidbody>();
    }

    public Transform Target;
    public override void OnEpisodeBegin()
    {
        if (this.transform.localPosition.y < 0)
        {
            this.rBody.angularVelocity = Vector3.zero;
            this.rBody.velocity = Vector3.zero;
            this.transform.localPosition = new Vector3(0, 0.5f, 0);
        }

        Target.localPosition = new Vector3(Random.value * 8 - 4, 0.5f, Random.value * 8 - 4);

    }

    public override void CollectObservations(VectorSensor sensor)
    {
        sensor.AddObservation(Target.localPosition);
        sensor.AddObservation(this.transform.localPosition);
        sensor.AddObservation(rBody.velocity.x);
        sensor.AddObservation(rBody.velocity.z);
    }
    public float forceMultiplier = 10;
    public override void OnActionReceived(ActionBuffers actionBuffers)
    {
        Vector3 controlSignal = Vector3.zero;
        controlSignal.x = actionBuffers.ContinuousActions[0];
        controlSignal.z = actionBuffers.ContinuousActions[1];
        rBody.AddForce(controlSignal * forceMultiplier);

        float distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.localPosition);

        if(distanceToTarget < 1.42f)
        {
            SetReward(1.0f);
            EndEpisode();
        }
        else if (this.transform.localPosition.y < 0)
        {
            EndEpisode();
        }
    }
}



```


-	В корень проекта добавим файл конфигурации нейронной сети.

![60](https://user-images.githubusercontent.com/104256775/209389769-d7da5cbc-5b6b-405e-9938-15ddb1857da8.png)

- Запустим ml-агента

-	Сделаем 3, 9, 27 копий модели «Плоскость-Сфера-Куб», запустим симуляцию сцены и выполним проверку.

![70](https://user-images.githubusercontent.com/104256775/209389801-ac1c9763-a81d-4204-897a-3d0a35662b21.png)

![71](https://user-images.githubusercontent.com/104256775/209389821-048070e9-e87b-40b8-9643-a328eafb778f.png)

![72](https://user-images.githubusercontent.com/104256775/209389834-0a4b5688-da81-4283-83aa-2b208a738514.png)

![73](https://user-images.githubusercontent.com/104256775/209389910-0ac80a16-a6c9-4053-97ed-096fb4fa6947.png)

![74](https://user-images.githubusercontent.com/104256775/209389934-2dcb929f-74d3-48b9-a340-d5050f3b6f41.png)

![75](https://user-images.githubusercontent.com/104256775/209389960-d8a9ea0d-43ab-48b2-8dc0-e79c3dddecb6.png)

![76](https://user-images.githubusercontent.com/104256775/209389989-b18afb94-36c4-4840-9401-04d82d47432d.png)

![77](https://user-images.githubusercontent.com/104256775/209390007-ee82a29c-31fd-458c-8ceb-24771e57560b.png)

![78](https://user-images.githubusercontent.com/104256775/209390033-fa4a2cdd-1482-43fd-9903-9d2d702b9109.png)

![79](https://user-images.githubusercontent.com/104256775/209390058-57a1f9e1-e39b-4481-a0a8-d922e72d834c.png)

 ## Задание 2. 
### Подробно опишите каждую строку файла конфигурации нейронной сети, доступного в папке с файлами проекта по ссылке. Самостоятельно найдите информацию о компонентах Decision Requester, Behavior Parameters, добавленных на сфере.

В файле `rollerball config.yaml` содержится YAML, в которой находятся все значения гиперпараметров. 

`behaviors`

 Основной раздел файла конфигурации тренера — это набор конфигураций для каждого действия на сцене.
 Параметры существующие в среде и описанные в нашем конфиге behaviors будут иметь значения, которые мы им присвоили в конфиге. 
  
`RollerBall`
	Когда MLAgent имеет статус active и пакет “mlagents==0.28.0” и фреймворк “torch~1.7.1” были загружены, сравниваем со стандартным обращением к агенту:
  
`mlagents-learn rollerball_config.yaml --run-id=RollerBall --resume`

или
	
`mlagents-learn <trainer-config-file> --env=<env_name> --run-id=<run-identifier>`

Нам нужно `rollerball_config.yaml` и id `RollerBall`
где `<run-identifier>` - имя, которое используется для идентификации результатов тренировок нейронной сети. Поскольку внутри одной среды может быть несколько объектов под контролем нейронной сети, они могут обучаться одновременно, но различным вещам. Например в то время, как RollerBall стремится к прикосновению с Target, Target может убегать от RollerBall. В таком случае нам не придется учить их отдельно, мы добавим раздел в конфигурацию обучения под id Target и внесем изменения в скрипт внутри Unity, где отметим результат “Сбежал” для нашего Target-кубика.


`trainer_type: ppo`
Сам параметр отвечает за тип используемого тренера (по умолчанию ppo). Proximal Policy Optimization [PPO] - класс алгоритмов обучения с подкреплением, которые работают сравнимо или даже лучше, чем современные подходы, но при этом их гораздо проще реализовать и настроить. 

`hyperparameters`
	Это раздел настройки гиперпараметров для агента. 

`batch_size`
	Количество опытов в каждой итерации градиентного спуска. Это всегда должно быть в несколько раз меньше, чем количество опытов для обновления модели политики(изменения поведенческой модели). Если мы используем непрерывные действия, это значение должно быть большим (порядка 1000 с). Но мы используем только дискретные действия, поэтому значение должно быть меньше (порядка 10 с).

`buffer_size`
	Количество опытов, которые необходимо собрать перед обновлением модели политики. Соответствует тому, сколько опыта должно быть собрано, прежде чем мы будем изучать или обновлять модель. Значение должно быть в несколько раз больше, чем batch_size. 

`learning_rate`
	Начальная скорость обучения для градиентного спуска. Соответствует силе каждого шага обновления градиентного спуска. 
	
`beta`
	beta - сила регуляризации энтропии, которая делает политику «более случайной». Это гарантирует, что агенты должным образом исследуют пространство действия во время обучения. Увеличение этого параметра обеспечит выполнение большего количества случайных действий. Если энтропия падает слишком быстро, увеличиваем beta. Если энтропия падает слишком медленно, уменьшаем beta.

`epsilon`
	Влияет на скорость изменения политики во время обучения. Соответствует допустимому порогу расхождения между старой и новой политикой при обновлении градиентного спуска. 
	
`lambd`
	Параметр регуляризации (лямбда), используемый при расчете обобщенной оценки преимущества ( GAE ). Лямбду можно рассматривать как то, насколько агент полагается на свою текущую оценку стоимости при вычислении обновленной оценки стоимости. 

`num_epoch`
	Количество проходов через буфер опыта при выполнении оптимизации градиентного спуска. Чем больше размер партии, тем больше это допустимо. Уменьшение этого параметра обеспечит более стабильные обновления за счет более медленного обучения.

`learning_rate_schedule`
	(по умолчанию = linear для PPO и constant для SAC) Определяет, как скорость обучения изменяется с течением времени. Для PPO в документации рекомендовано снижать скорость обучения до значения max_steps, чтобы обучение сходилось более стабильно. 

`linear` 
	скорость обучения уменьшается линейно, достигая 0 на max_steps, сохраняя при constant этом скорость обучения постоянной для всего тренировочного прогона.

`network_settings`
	Раздел, содержащий настройки нейронной сети.

`normalize`
	Параметр определяет применяется ли нормализация к входным данным векторных наблюдений. Нормализация может быть полезна в случаях со сложными задачами непрерывного управления, но может быть вредна для более простых задач дискретного управления. По умолчанию false.

`hidden_units`
	Количество единиц в скрытых слоях нейронной сети. Соответствуют количеству единиц в каждом полносвязном слое нейронной сети. 

`num_layers`
	Количество скрытых слоев в нейронной сети. Соответствует количеству скрытых слоев после ввода наблюдения или после кодирования CNN визуального наблюдения. 

`reward_signals`
	Раздел позволяет задавать настройки как для внешних (т. е. основанных на среде), так и для внутренних сигналов вознаграждения (например, любопытство и GAIL). 

`extrinsic`
	Внешние сигналы вознаграждения(основанные на среде)
 
`gamma`
	(по умолчанию = 0.99) Фактор скидки для будущих вознаграждений, поступающих из окружающей среды. Это можно рассматривать как то, как далеко в будущем агент должен заботиться о возможных вознаграждениях. 

`strength`
(по умолчанию = 1.0) Коэффициент, на который умножается вознаграждение, данное средой. Диапазоны будут варьируются в зависимости от сигнала вознаграждения.

`max_steps`
Общее количество шагов (т. е. собранных наблюдений и предпринятых действий), которые необходимо выполнить в среде (или во всех средах при параллельном использовании нескольких) перед завершением процесса обучения. 

`time_horizon`
Сколько шагов опыта необходимо собрать для каждого агента, прежде чем добавить его в буфер опыта. Когда этот предел достигается до конца эпизода, оценка значения используется для прогнозирования общего ожидаемого вознаграждения из текущего состояния агента. 

`summary_freq`
(по умолчанию = 50000) Количество опытов, которое необходимо собрать перед созданием и отображением статистики обучения. Это определяет детализацию графиков в Tensorboard.

### Компоненты	

- `Decision Requester`
Цель обучения с подкреплением — изучить наилучшую политику (сопоставление состояний с действиями), которая максимизирует возможные вознаграждения. Компонент “Decision Requester” выбирает, какое действие будет принято в данном эпизоде обучения.

- `Decision Period`
Отвечает за частоту опроса решений из базы знаний агента. 

- `Take Actions Between Decision Period`
Будет ли агент принимать решения во время отработки действий из базы знаний.

- `Behavior Parameters`
В среде RollerBall базовый объект RollerAgent имеет несколько свойств, влияющих на его поведение.
Параметры поведения — у каждого агента должно быть поведение. Поведение определяет, как агент принимает решения.

- `Stacked Vectors`
Количество состояний, которые будут склаываться перед подачей в нейронную сеть.

- `Continuous Actions`
Количество непрерывных действий.

- Discrete Branches`
Кол-во дискретных ветвей (наборов событий для получения награды).

- `Model` 
Обученная модель поведения, вкладывается после успешного обучения агента.

- `Interface Device`
Устройство для генерации поведения объекта (под управлением нейронной сети).

- `Behavior Type`
Отвечает за функционал агента. Например, невозможно обучить агента, если данное поле имеет параметр только вывод, поскольку он подразумевает использование накопленной информации.

- `Team ID`
Параметр для выбора команды агента или ИИ. Например, если мы делаем спортивный симулятор, то мы будем обучать агентов одному и тому же, но играть они будут против друг друга.

- `Observable Attribute Hand`
Изучение агентом атрибутов. Может исключаться, изучить все или только дочерние атрибуты для тела под управлением агента.



## Выводы
Я сумел реализовать обучение машины в Unity, обучив MLAgent с использованием библиотек torch 1.7.1 и mlagents 0.28.9.
Для выполнения задания №2 я изучил документацию по созданию тренера агента.
